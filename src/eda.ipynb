{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b155ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "CSV_PATH =os.getenv(\"CSV_PATH\")\n",
    "\n",
    "df=pd.read_csv(CSV_PATH)\n",
    "df.dropna(subset=[\"Message\"], inplace=True)\n",
    "df = df[df[\"Auteur\"] != \"Inconnu\"]\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe329a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Ajouter une colonne 'lang'\n",
    "df[\"lang\"] = df[\"Message\"].apply(detect_lang)\n",
    "\n",
    "# Garder uniquement les messages dÃ©tectÃ©s comme anglais\n",
    "df = df[df[\"lang\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80382efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = {\n",
    "    'im', 'ive', 'ya', 'hi', 'hello', 'everyone', 'guy', 'guys', 'd', 'would', 'like', 'one',\n",
    "    'get', 'also', 'know', 'dont', 'let', 'us', 'want', 'need', 'use', 'using', 'question',\n",
    "    'help', 'thank', 'thanks', 'thing', 'things', 'hey', 'still', 'well', 'maybe', 'look',\n",
    "    'looking','anyone', 'someone', 'please', 'make', 'good', 'work', 'time', 'really', 'trying', 'interested',\n",
    "      'youre', 'take', 'find', 'something','he','great','un', 'ce', 'dans','new','chaneel','talk','le', 'tous',\n",
    "    'st','fine','frind','d',\n",
    " }\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).strip()\n",
    "    text = \" \".join(text.split())\n",
    "    text=text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  \n",
    "    text = re.sub(r\"@\\w+\", \"\", text) \n",
    "    text = re.sub(r\"#\\w+\", \"\", text)      \n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    stop_words = set(stopwords.words(\"english\")).union(custom_stopwords)\n",
    "    word_tokens = word_tokenize(text) \n",
    "    text = [word for word in word_tokens if word not in stop_words and len(word) > 1]\n",
    "    if len(text) < 3:\n",
    "        return \"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text=[lemmatizer.lemmatize(word) for word in text]         \n",
    "    return \" \".join(text)\n",
    "\n",
    "df[\"cleaned\"] = df[\"Message\"].apply(clean_text)\n",
    "df = df[df[\"cleaned\"] != \"\"] \n",
    "df[\"Autclean\"] = df[\"Auteur\"].str.replace(\"@\", \"\", regex=False)\n",
    "df.shape\n",
    "df.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3eb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['Message', 'cleaned']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ•µï¸â€â™‚ï¸ 1. Emoji le plus frÃ©quent\n",
    "emoji_counts = df['Emoji'].value_counts()\n",
    "Emoji_le_plus_utilisÃ© = emoji_counts.idxmax()\n",
    "\n",
    "print(\"Emoji le plus utilisÃ© :\", Emoji_le_plus_utilisÃ©)\n",
    "print(emoji_counts.head(10)) \n",
    "\n",
    "\n",
    "# ðŸ‘¤ 2. Utilisateurs les plus actifs\n",
    "user_counts = df['Autclean'].value_counts()\n",
    "most_active_user = user_counts.idxmax()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x=user_counts.head(10).index, y=user_counts.head(10).values, palette=\"crest\")\n",
    "plt.title(\"Top 10 des utilisateurs les plus actifs\")\n",
    "plt.xlabel(\"Utilisateur\")\n",
    "plt.ylabel(\"Nombre de messages\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "plt.savefig(\"Top 10 des utilisateurs les plus actifs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b5be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.setlocale(locale.LC_TIME, 'French_France')\n",
    "\n",
    "df[\"Date_clean\"] = df[\"Date\"].str.replace(\"Ã \", \"\", regex=False).str.strip()\n",
    "\n",
    "# Conversion en datetime\n",
    "df[\"Date_clean\"] = pd.to_datetime(df[\"Date_clean\"], format=\"%d %B %Y %H:%M\", errors=\"coerce\")\n",
    "\n",
    "df[\"heure\"] = df[\"Date_clean\"].dt.hour\n",
    "frequence_par_heure = df[\"heure\"].value_counts().sort_index()\n",
    "\n",
    "# Tracer\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=frequence_par_heure.index, y=frequence_par_heure.values, palette=\"viridis\")\n",
    "plt.title(\"Nombre d'activitÃ©s par heure\")\n",
    "plt.xlabel(\"Heure de la journÃ©e\")\n",
    "plt.ylabel(\"Nombre d'activitÃ©s\")\n",
    "plt.xticks(range(24))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"frequence_par_heure.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f96cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Usage d'emojis par utilisateur (top 10)\n",
    "emoji_counts_by_user = (\n",
    "    df[df['Emoji'].astype(bool)]\n",
    "      .groupby('Autclean')['Emoji']\n",
    "      .count()\n",
    "      .sort_values(ascending=False)\n",
    "      .head(10)\n",
    ")\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=emoji_counts_by_user.values, y=emoji_counts_by_user.index)\n",
    "plt.title(\"Top 10 des utilisateurs ayant envoyÃ© le plus d'emojis\")\n",
    "plt.xlabel(\"Nombre d'emojis\")\n",
    "plt.ylabel(\"Utilisateur\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"emoji_par_utilisateur.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "all_words = []\n",
    "for text in df[\"cleaned\"]: \n",
    "    all_words.extend(text.split())\n",
    "\n",
    "freq = Counter(all_words)\n",
    "\n",
    "common_words = freq.most_common(30)\n",
    "print(common_words)\n",
    "\n",
    "# 4. Visualisation : barplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([word for word, count in common_words], [count for word, count in common_words], color='skyblue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 20 mots les plus frÃ©quents\")\n",
    "plt.xlabel(\"Mots\")\n",
    "plt.ylabel(\"FrÃ©quence\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"Top 20 mots les plus frÃ©quents\")\n",
    "\n",
    "#nuage des mots\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(all_words))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Nuage de mots\")\n",
    "plt.show()\n",
    "plt.savefig(\"Nuage de mots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3090e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text.split() for text in df[\"cleaned\"]]\n",
    "\n",
    "# CrÃ©ation de bigrammes\n",
    "bigram = Phrases(texts, min_count=5, threshold=30)\n",
    "bigram_mod = Phraser(bigram)\n",
    "texts_bigram = [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# Dictionnaire et corpus\n",
    "id2word = corpora.Dictionary(texts_bigram)\n",
    "id2word.filter_extremes(no_below=5, no_above=0.6)\n",
    "corpus = [id2word.doc2bow(text) for text in texts_bigram]\n",
    "\n",
    "# Fonction pour calculer la cohÃ©rence\n",
    "def compute_coherence_values(dictionary, corpus, texts, start, limit, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step): \n",
    "        model = models.LdaModel(corpus=corpus,\n",
    "                                id2word=dictionary,\n",
    "                                num_topics=num_topics,\n",
    "                                random_state=100,\n",
    "                                update_every=1,\n",
    "                                chunksize=100,\n",
    "                                passes=10,\n",
    "                                alpha='auto',\n",
    "                                per_word_topics=False)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Tester plusieurs nombres de topics\n",
    "start, limit, step = 5, 30, 1\n",
    "model_list, coherence_values = compute_coherence_values(id2word, corpus, texts_bigram, start, limit, step)\n",
    "\n",
    "# Figure de score de cohÃ©rence\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Coherence Score vs Number of Topics\")\n",
    "plt.show()\n",
    "plt.savefig(\"Coherence Score vs Number of Topics\")\n",
    "\n",
    "# Meilleur modÃ¨le\n",
    "best_index = coherence_values.index(max(coherence_values))\n",
    "best_model = model_list[best_index]\n",
    "best_num_topics = start + best_index * step\n",
    "print(f\" Meilleur nombre de topics : {best_num_topics} avec coherence = {coherence_values[best_index]:.4f}\")\n",
    "pprint(best_model.print_topics())\n",
    "\n",
    "\n",
    "# Assigner les topics au DataFrame\n",
    "df[\"topic\"] = [max(best_model[doc], key=lambda x: x[1])[0] for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e785af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "pyldavis_prepared = gensimvis.prepare(best_model, corpus, id2word)\n",
    "pyLDAvis.display(pyldavis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiments\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "vaders = pd.DataFrame([sia.polarity_scores(text) for text in df['Message']])\n",
    "df_with_sentiment = pd.concat([df.reset_index(drop=True), vaders], axis=1)\n",
    "df_with_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d1c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"topic\", y=\"compound\", data=df_with_sentiment, palette=\"viridis\")\n",
    "plt.title(\"Sentiment moyen par Topic\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Score de sentiment\")\n",
    "plt.show()\n",
    "plt.savefig(\"Sentiment moyen par Topic\")\n",
    "\n",
    "sns.boxplot(x=\"topic\", y=\"compound\", data=df_with_sentiment, palette=\"viridis\")\n",
    "plt.title(\"Distribution des sentiments par topic\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Score de sentiment\")\n",
    "plt.show()\n",
    "plt.savefig(\"Distribution des sentiments par topic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
